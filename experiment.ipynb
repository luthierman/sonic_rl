{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sonic_env\n",
    "\n",
    "env = sonic_env.make(\"SonicTheHedgehog-Genesis\", \"GreenhillZone.Act1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 320, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expecting input height: 84, got: 320",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\DAVIDH~1\\AppData\\Local\\Temp/ipykernel_8024/2068521772.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mModel\u001B[0m\u001B[1;33m(\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobservation_space\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction_space\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\sonic_rl\\models.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, input_dim, output_dim)\u001B[0m\n\u001B[0;32m     19\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_actions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moutput_dim\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m84\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Expecting input height: 84, got: {self.input_shape[1]}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m84\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Expecting input width: 84, got: {self.input_shape[2]}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Expecting input height: 84, got: 320"
     ]
    }
   ],
   "source": [
    "Model( env.observation_space.shape,env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import *\n",
    "import torch\n",
    "from collections import deque\n",
    "from hyper_parameters import *\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from experience_replay import *\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self ,env, action_space, state_space) -> None:\n",
    "        # GYM environment\n",
    "        self.env = env\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "\n",
    "        # HYPERPARAMETERS\n",
    "        self.lr = LR\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = EPSILON\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        self.epsilon_min = EPSILON_MIN\n",
    "        self.batch = BATCH_SIZE\n",
    "        self.episodes = N_EPISODES\n",
    "        # Q-network and Target-network\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = Model()\n",
    "        print(\"Q-NETWORK:\\n\", self.q_network)\n",
    "        self.opt = Adam(self.q_network.parameters(), lr=self.lr)\n",
    "        self.target = Dense_Net()\n",
    "        self.sync_weights()\n",
    "        if use_cuda:\n",
    "            print(\"GPU being used:\", torch.cuda.get_device_name(0))\n",
    "            self.q_network.cuda(self.device)\n",
    "            self.target.cuda(self.device)\n",
    "        self.target.eval()\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        # DQN setup\n",
    "        self.buff = 2000\n",
    "        self.memory = ER_Memory(self.buff)\n",
    "        self.counter = 0\n",
    "        self.update_target = 1\n",
    "        self.step = 0\n",
    "        self.train_start = 1\n",
    "        self.current_episode = 0\n",
    "        self.lambda1 = lambda epoch: 0.999999999 ** epoch\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.opt, lr_lambda=self.lambda1)\n",
    "\n",
    "        # stat tracking\n",
    "        self.rewards = []\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.q_values = []\n",
    "        self.episode_times = []\n",
    "        self.episode_time = 0\n",
    "        self.no_steps = []\n",
    "        self.avg = 0\n",
    "        self.total_reward = 0\n",
    "        self.avg_reward = deque(maxlen=self.episodes)\n",
    "        self.rs = deque(maxlen=50)\n",
    "        self.lrs = []\n",
    "\n",
    "        # save model\n",
    "        self.log_path = os.getcwd() + \"\\logs\"\n",
    "        self.name = \"DQN_Dense_{}\".format(datetime.datetime.now())\n",
    "\n",
    "    def preprocess_state(self, x):\n",
    "        state = np.stack(x)\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        return state\n",
    "\n",
    "    def run_episode(self):\n",
    "        start_time = time.time()\n",
    "        s1 = self.env.reset()\n",
    "        steps = 0\n",
    "        done = False\n",
    "        self.total_reward = 0\n",
    "        total_loss = 0\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = self.get_action(s1)\n",
    "\n",
    "            s2, reward, done, _ = self.env.step(action)\n",
    "            self.total_reward += reward\n",
    "            self.remember(s1,\n",
    "                          action,\n",
    "                          reward,\n",
    "                          s2,\n",
    "                          done)\n",
    "            if done:\n",
    "                self.episode_time = time.time() - start_time\n",
    "                self.episode_times.append(self.episode_time)\n",
    "                self.rewards.append(self.total_reward),\n",
    "                self.losses.append(total_loss),\n",
    "                self.rs.append(self.total_reward)\n",
    "                self.no_steps.append(steps)\n",
    "                self.avg = np.mean(self.rs)\n",
    "                self.avg_reward.append(self.avg)\n",
    "                self.current_episode += 1\n",
    "                break\n",
    "            s1 = s2\n",
    "            total_loss += self.learn()\n",
    "\n",
    "            steps += 1\n",
    "            if steps % 10 == 0:\n",
    "                self.sync_weights()\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return 0\n",
    "\n",
    "        minibatch = self.memory.sample(min(len(self.memory), self.batch))\n",
    "        states = self.preprocess_state(minibatch[:, 0])\n",
    "        actions = self.preprocess_state(minibatch[:, 1]).type(torch.int64).unsqueeze(-1)\n",
    "        rewards = self.preprocess_state(minibatch[:, 2])\n",
    "        next_states = self.preprocess_state(minibatch[:, 3])\n",
    "        dones = self.preprocess_state(minibatch[:, 4])\n",
    "        # DQN\n",
    "\n",
    "        self.q_network.train()\n",
    "        self.target.eval()\n",
    "        # Q\n",
    "        Q = self.q_network.forward(states).gather(1, actions).squeeze(-1)  # Q(s, a, wq)\n",
    "        # target\n",
    "        Q_next = self.target.forward(next_states).max(1)[0].detach()  # max _a Q(ns, a, wt)\n",
    "        y = rewards + self.gamma * (1 - dones) * Q_next  # bellman\n",
    "        self.opt.zero_grad()\n",
    "        loss = self.loss_fn(y, Q)\n",
    "        loss.backward()\n",
    "        for param in self.q_network.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.opt.step()\n",
    "\n",
    "        self.lrs.append(self.opt.param_groups[0][\"lr\"])\n",
    "        self.scheduler.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_weights(self):\n",
    "        self.target.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if len(self.memory) > self.batch:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            self.q_network.eval()\n",
    "            obs = self.preprocess_state([obs])\n",
    "            return self.q_network(obs).argmax().item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.remember(state, action, reward, next_state, done)\n",
    "        self.counter += 1\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "agent_dqn = DQN()\n",
    "print(agent_dqn.name)\n",
    "for i in range(agent_dqn.episodes):\n",
    "    agent_dqn.run_episode()\n",
    "    print(\"\\rEpisode {}/{} [{} sec.]|| Current Avg {}, Episode Reward {}, Steps {}, eps {}\".format(\n",
    "        agent_dqn.current_episode,\n",
    "        agent_dqn.episodes,\n",
    "        np.round(agent_dqn.episode_time, 3),\n",
    "        agent_dqn.avg,\n",
    "        agent_dqn.total_reward,\n",
    "        agent_dqn.no_steps[i],\n",
    "        agent_dqn.epsilon\n",
    "    ), flush=True, end=\"\")\n",
    "\n",
    "plt.plot(np.arange(1,201), agent_dqn.rewards)\n",
    "plt.plot(np.arange(1,201), agent_dqn.avg_reward)\n",
    "\n",
    "plt.title(agent_dqn.name)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}